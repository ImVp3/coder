{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a2ade7",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18abd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_ANALYSIS_TEMPLATE = \"\"\"\n",
    "You are an expert code analyst. Analyze the following Python code and provide a structured summary\n",
    "that will be useful for generating unit tests. Identify:\n",
    "1.  Main functions and classes.\n",
    "2.  For each function/method:\n",
    "    - Its purpose or a brief description.\n",
    "    - Input parameters (name and type if inferable).\n",
    "    - Return type (if inferable).\n",
    "    - Key logic branches or behaviors.\n",
    "    - Potential edge cases or interesting scenarios to test.\n",
    "3.  Any global variables or external dependencies that might affect testing.\n",
    "\n",
    "Respond in a JSON format with the following structure:\n",
    "{{  \n",
    "    \"summary\": \"Overall summary of the code's purpose.\",\n",
    "    \"components\": [\n",
    "    {{  \n",
    "        \"type\": \"function\" | \"class\",\n",
    "        \"name\": \"component_name\",\n",
    "        \"description\": \"...\",\n",
    "        \"methods\": [ // Only if type is class\n",
    "        {{  \n",
    "            \"name\": \"method_name\",\n",
    "            \"signature\": \"def method_name(param1, param2): ...\",\n",
    "            \"description\": \"...\",\n",
    "            \"parameters\": [{{\"name\": \"param_name\", \"type\": \"inferred_type\"}}, ...], \n",
    "            \"returns\": \"inferred_return_type\",\n",
    "            \"key_behaviors\": [\"behavior1\", \"behavior2\"],\n",
    "            \"edge_cases\": [\"edge_case1\", ...]\n",
    "        }} \n",
    "        ],\n",
    "        // For functions directly under components, structure similar to methods above\n",
    "        \"signature\": \"def function_name(param1): ...\", // If type is function\n",
    "        \"parameters\": [...], // If type is function\n",
    "        \"returns\": \"...\", // If type is function\n",
    "        \"key_behaviors\": [...], // If type is function\n",
    "        \"edge_cases\": [...] // If type is function\n",
    "    }} \n",
    "    ],\n",
    "    \"dependencies\": [\"dep1\", \"dep2\"]\n",
    "}} \n",
    "Remember to include JSON strings without any extra formatting or signs.\n",
    "Code to analyze:\n",
    "{code_to_analyze}\n",
    "\"\"\"\n",
    "\n",
    "TEST_GENERATION_TEMPLATE= \"\"\"\n",
    "You are an expert Python test developer. Based on the following analysis of a Python code component\n",
    "and the original code context, write comprehensive unit tests using the `unittest` framework.\n",
    "\n",
    "Original Code Context (for reference, ensure your tests would import/access this correctly):\n",
    "```python\n",
    "{original_code_snippet}\n",
    "```\n",
    "\n",
    "Code Component Analysis:\n",
    "Name: {component_name}\n",
    "Type: {component_type}\n",
    "Signature: `{component_signature}`\n",
    "Description: {component_description}\n",
    "Key Behaviors to Test: {key_behaviors}\n",
    "Potential Edge Cases to Test: {edge_cases}\n",
    "\n",
    "{feedback}\n",
    "\n",
    "Your task:\n",
    "1.  Create a Python class that inherits from `unittest.TestCase`. Name it descriptively.\n",
    "2.  Write test methods (starting with `test_`) within this class.\n",
    "3.  Each test method should target one behavior or edge case identified.\n",
    "4.  Use appropriate `self.assertXXX` methods from `unittest` for assertions.\n",
    "5.  Ensure the tests are self-contained and clearly written.\n",
    "6.  Assume necessary functions/classes from the original code are importable or accessible in the test execution scope.\n",
    "    (For example, if testing a function `my_function` from `original_code`, your test might call `source_module.my_function(...)`\n",
    "    or assume `my_function` is directly available if `original_code` was executed in the global scope of tests.\n",
    "    For now, assume the component `{component_name}` is directly callable/instantiable.)\n",
    "7.  Do NOT include the `if __name__ == '__main__': unittest.main()` block.\n",
    "8.  Only provide the Python code for the test class. Do not add any explanatory text before or after the code block.\n",
    "\n",
    "Component to generate tests for: `{component_name}`\n",
    "Test Class Code:\n",
    "```python\n",
    "# [Your generated unittest.TestCase class for {component_name} goes here]\n",
    "```\n",
    "\"\"\"\n",
    "EVALUATION_TEMPLATE= \"\"\"\n",
    "You are an expert Senior QA Engineer and Python Developer. Your task is to review a suite of generated unit tests\n",
    "against the original Python code and its prior analysis. You should assess the quality, completeness,\n",
    "and likely effectiveness of these tests. DO NOT execute the code.\n",
    "\n",
    "Original Python Code:\n",
    "```python\n",
    "{original_code}\n",
    "```\n",
    "\n",
    "Prior Code Analysis (identifying key components, behaviors, and edge cases that should be tested):\n",
    "```json\n",
    "{code_analysis_json}\n",
    "```\n",
    "Generated Unit Tests (using unittest framework):\n",
    "```python\n",
    "{test_code}\n",
    "```\n",
    "\n",
    "Based on your review of these three inputs, please provide:\n",
    "1.  An overall qualitative assessment of the test suite's likely coverage and quality. Choose one: \"low\", \"medium\", \"high\".\n",
    "2.  A numeric score from 1 (very poor) to 10 (excellent) representing your confidence in these tests.\n",
    "3.  Specific feedback:\n",
    "    - What aspects of the original code (based on the analysis) seem well-tested?\n",
    "    - What specific functions, methods, logic paths, or edge cases (from the analysis or your own observation of the original code) appear to be untested or inadequately tested by the provided test suite?\n",
    "    - Any other suggestions for improving these tests (e.g., missing assertions, incorrect mocking (if inferable), unclear test names, testing anti-patterns).\n",
    "\n",
    "Respond in a JSON format with the following structure:\n",
    "{{\n",
    "    \"qualitative_assessment\": \"low|medium|high\",\n",
    "    \"confidence_score\": <float_from_1_to_10>,\n",
    "    \"positive_feedback\": [\"Aspect 1 well-tested...\", \"Aspect 2...\"],\n",
    "    \"areas_for_improvement\": [\"Function X seems untested for Y case...\", \"Edge case Z is missing...\"],\n",
    "    \"other_suggestions\": [\"Consider testing X...\", \"Test Y could be clearer if...\"]\n",
    "}}\n",
    "Remember to include JSON strings without any extra formatting or signs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df08b9",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d465964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import MessagesState\n",
    "from typing_extensions import TypedDict, List, Dict, Any, Optional, Annotated\n",
    "import operator\n",
    "\n",
    "class UnitTestWorkflowState(MessagesState):\n",
    "    original_code: str\n",
    "    analyzed_code: str\n",
    "    test_code: str\n",
    "    evaluation: dict\n",
    "    flow: Annotated[List[str], operator.add]\n",
    "    max_generation_attempts: int\n",
    "    generation_attempts: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b83b8",
   "metadata": {},
   "source": [
    "# Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4253f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal, Union, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Parameter(BaseModel):\n",
    "    name: str \n",
    "    type: Optional[str] = None \n",
    "\n",
    "class MethodDetails(BaseModel):\n",
    "    signature: Optional[str] = None \n",
    "    description: Optional[str] = None \n",
    "    parameters: List[Parameter] = Field(default_factory=list)\n",
    "    returns: Optional[str] = None\n",
    "    key_behaviors: List[str] = Field(default_factory=list) \n",
    "    edge_cases: List[str] = Field(default_factory=list)\n",
    "\n",
    "class Method(MethodDetails):\n",
    "    name: str \n",
    "\n",
    "class FunctionComponent(MethodDetails):\n",
    "    type: Literal[\"function\"] \n",
    "    name: str \n",
    "\n",
    "class ClassComponent(BaseModel):\n",
    "    type: Literal[\"class\"] \n",
    "    name: str \n",
    "    description: Optional[str] = None \n",
    "    methods: List[Method] = Field(default_factory=list) \n",
    "\n",
    "Component = Union[FunctionComponent, ClassComponent]\n",
    "\n",
    "class CodeAnalysis(BaseModel):\n",
    "    summary: Optional[str] = None \n",
    "    components: List[Component] = Field(default_factory=list, discriminator='type' if hasattr(Field, 'discriminator') else None) # Mặc định là danh sách rỗng\n",
    "    dependencies: List[str] = Field(default_factory=list) \n",
    "    \n",
    "class TestCodeEvaluation(BaseModel):\n",
    "    qualitative_assessment: Literal[\"low\", \"medium\", \"high\"]\n",
    "    confidence_score: float \n",
    "    positive_feedback: List[str] = Field(default_factory=list) \n",
    "    areas_for_improvement: List[str] = Field(default_factory=list) \n",
    "    other_suggestions: List[str] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64899e3",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6237bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Code\\coder\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "\n",
    "def get_model(model: str, temperature: float = 0.0) -> ChatGoogleGenerativeAI | ChatOpenAI:\n",
    "    if \"gemini\" in model:\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=model,\n",
    "            temperature=temperature\n",
    "        )\n",
    "    if \"gpt\" in model:\n",
    "        return ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature\n",
    "        )\n",
    "    raise ValueError(f\"Model {model} not supported\")\n",
    "\n",
    "def create_code_analysis_chain(\n",
    "    model: str,\n",
    "    temperature: float = 0.0,\n",
    ") -> LLMChain:\n",
    "    llm = get_model(model, temperature)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "    CODE_ANALYSIS_TEMPLATE\n",
    "    )\n",
    "    chain = prompt | llm |JsonOutputParser(pydantic_object=CodeAnalysis)\n",
    "    return chain\n",
    "def create_test_generation_chain(\n",
    "    model: str,\n",
    "    temperature: float = 0.0,\n",
    ") -> LLMChain:\n",
    "    llm = get_model(model, temperature)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        TEST_GENERATION_TEMPLATE\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    return chain\n",
    "def create_evaluation_chain(\n",
    "    model: str,\n",
    "    temperature: float = 0.0,\n",
    ") -> LLMChain:\n",
    "    llm = get_model(model, temperature)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        EVALUATION_TEMPLATE\n",
    "    )\n",
    "    chain = prompt | llm | JsonOutputParser(pydantic_object=CodeAnalysis)\n",
    "    return chain\n",
    "def decision_to_end_workflow(state: UnitTestWorkflowState) -> str:\n",
    "    if state[\"generation_attempts\"] >= state[\"max_generation_attempts\"] or state[\"evaluation\"][\"qualitative_assessment\"] == \"high\":\n",
    "        return \"end\"\n",
    "    return \"regenerate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b4c61",
   "metadata": {},
   "source": [
    "# Node Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7910ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde66e48",
   "metadata": {},
   "source": [
    "## Code Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505a79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict, Any, List, Optional\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage , AIMessage\n",
    "\n",
    "# TODO: import UnitTestWorkflowState\n",
    "\n",
    "# --- Constants cho analyze_code_node ---\n",
    "ANALYZE_NODE_NAME = \"Analyze Code Node\"\n",
    "ANALYZE_FLOW_SUCCESS = f\"{ANALYZE_NODE_NAME}: Analysis successful\"\n",
    "\n",
    "ANALYZE_FLOW_FAILED_NO_CODE = f\"{ANALYZE_NODE_NAME}: Failed (No original code provided)\"\n",
    "ANALYZE_FLOW_FAILED_LLM = f\"{ANALYZE_NODE_NAME}: Failed (LLM analysis error)\"\n",
    "\n",
    "MESSAGE_NO_ORIGINAL_CODE = \"No original code found in state for analysis.\"\n",
    "MESSAGE_FLOW_FAILED_LLM = \"The original code provided has a syntax error, which prevents analysis.\"\n",
    "\n",
    "def code_analysis(state: UnitTestWorkflowState, code_analysis_chain) -> UnitTestWorkflowState:\n",
    "  \"\"\"\n",
    "  Analyze the code and provide a structured summary for generating unit tests.\n",
    "  \"\"\"\n",
    "  original_code = state.get(\"original_code\",None)\n",
    "  if not original_code:\n",
    "      return {\n",
    "        \"messages\": [MESSAGE_NO_ORIGINAL_CODE],\n",
    "        \"flow\": [ANALYZE_FLOW_FAILED_NO_CODE]\n",
    "      }\n",
    "  analyzed_code = code_analysis_chain.invoke({\n",
    "      \"code_to_analyze\": original_code\n",
    "  })\n",
    "  if not analyzed_code:\n",
    "      return {\n",
    "        \"messages\": [MESSAGE_FLOW_FAILED_LLM],\n",
    "        \"flow\": [ANALYZE_FLOW_FAILED_LLM]\n",
    "      }\n",
    "  return {\n",
    "    \"analyzed_code\": analyzed_code,\n",
    "    \"messages\": [AIMessage(content=str(analyzed_code))],\n",
    "    \"flow\": [ANALYZE_FLOW_SUCCESS]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d63c27",
   "metadata": {},
   "source": [
    "## generate tests node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062d8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage\n",
    "import json\n",
    "# (Giả sử UnitTestWorkflowState đã được định nghĩa ở đâu đó)\n",
    "# from .state import UnitTestWorkflowState\n",
    "\n",
    "# --- Constants cho generate_tests_node ---\n",
    "GENERATE_NODE_NAME = \"Generate Tests Node\"\n",
    "GENERATE_FLOW_SUCCESS = f\"{GENERATE_NODE_NAME}: Test generation successful\"\n",
    "GENERATE_FLOW_FAILED_NO_ANALYSIS = f\"{GENERATE_NODE_NAME}: Failed (No code analysis found)\"\n",
    "GENERATE_FLOW_FAILED_INVALID_ANALYSIS = f\"{GENERATE_NODE_NAME}: Failed (Invalid code analysis format)\"\n",
    "GENERATE_FLOW_FAILED_LLM = f\"{GENERATE_NODE_NAME}: Failed (LLM test generation error)\"\n",
    "GENERATE_FLOW_FAILED_SYNTAX_IN_TESTS = f\"{GENERATE_NODE_NAME}: Failed (Generated tests have syntax errors)\"\n",
    "GENERATE_FLOW_FAILED_UNEXPECTED = f\"{GENERATE_NODE_NAME}: Failed (Unexpected Error)\"\n",
    "\n",
    "MESSAGE_NO_CODE_ANALYSIS = \"No code analysis found in state. Cannot generate tests.\"\n",
    "MESSAGE_INVALID_ANALYSIS_FORMAT = \"Code analysis data is missing 'components' or has an invalid format.\"\n",
    "MESSAGE_SYNTAX_ERROR_IN_GENERATED_TESTS = \"The LLM-generated test code has a syntax error.\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_tests_node(state: UnitTestWorkflowState, test_generation_chain ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates unit test code based on the code analysis provided.\n",
    "    Uses an LLM to write test cases for identified components.\n",
    "    \"\"\"\n",
    "\n",
    "    analyzed_code: Optional[Dict[str, Any]] = state.get('analyzed_code')\n",
    "    original_code: Optional[str] = state.get('original_code') \n",
    "    generation_attempts: int = state.get('generation_attempts', 0)    \n",
    "    \n",
    "    if not analyzed_code:\n",
    "        error_system_message = SystemMessage(content=MESSAGE_NO_CODE_ANALYSIS)\n",
    "        return {\n",
    "            \"messages\": [error_system_message],\n",
    "            \"flow\": [GENERATE_FLOW_FAILED_NO_ANALYSIS],\n",
    "        }\n",
    "\n",
    "    components_to_test = analyzed_code.get(\"components\")\n",
    "    if not components_to_test or not isinstance(components_to_test, list):\n",
    "        error_system_message = SystemMessage(content=MESSAGE_INVALID_ANALYSIS_FORMAT)\n",
    "        return {\n",
    "            \"messages\":  [error_system_message],\n",
    "            \"flow\":  [GENERATE_FLOW_FAILED_INVALID_ANALYSIS],\n",
    "        }\n",
    "    last_test_code = state.get(\"test_code\", \"\")\n",
    "    last_evaluation = json.dumps(state.get(\"evaluation\"))\n",
    "    if last_test_code and last_evaluation:\n",
    "        feedback_msg = f\"Previous generated test code:\\n{last_test_code}\\n\\nPrevious evaluation for the test code:\\n{last_evaluation}\"\n",
    "    else:\n",
    "        feedback_msg = \"\"          \n",
    "    \n",
    "    test_codes = []\n",
    "\n",
    "    for component in components_to_test:\n",
    "        try:\n",
    "            test_code = test_generation_chain.invoke({\n",
    "                \"original_code_snippet\": original_code,\n",
    "                \"component_name\": component[\"name\"],\n",
    "                \"component_type\": component[\"type\"],\n",
    "                \"component_signature\": component.get(\"signature\", \"\"),\n",
    "                \"component_description\": component.get(\"description\", \"\"),\n",
    "                \"key_behaviors\": component.get(\"key_behaviors\", []),\n",
    "                \"edge_cases\": component.get(\"edge_cases\", []),\n",
    "                \"feedback\": feedback_msg\n",
    "            })\n",
    "            test_codes.append(test_code.content)\n",
    "        except Exception as e:\n",
    "            error_system_message = SystemMessage(\n",
    "                content=f\"Error generating tests for component {component['name']}: {type(e).__name__} - {e}\"\n",
    "                )\n",
    "            return {\n",
    "                \"messages\": [error_system_message],\n",
    "                \"flow\": [GENERATE_FLOW_FAILED_LLM],\n",
    "            }\n",
    "\n",
    "    if not test_codes:\n",
    "        # This case means no components to test, or LLM failed for all without throwing an exception caught above\n",
    "        no_tests_msg = \"No test classes were generated. Check code analysis or LLM responses.\"\n",
    "        error_system_message = SystemMessage(content=no_tests_msg)\n",
    "        return {\n",
    "            \"messages\": [error_system_message],\n",
    "            \"flow\": [GENERATE_FLOW_FAILED_LLM], \n",
    "        }\n",
    "\n",
    "    # Combine all generated test classes and necessary imports\n",
    "    all_generated_tests_str =  \"\\n\\n\".join(test_codes)\n",
    "    all_generated_tests_str += \"\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n\"\n",
    "    all_generated_tests_str += \"# e.g., if __name__ == '__main__': unittest.main()\\n\"\n",
    "    # NOTE: But for programmatic use (like coverage.py), this main block is often not needed or added later.\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=all_generated_tests_str)],\n",
    "        \"test_code\": [all_generated_tests_str],\n",
    "        \"flow\": [GENERATE_FLOW_SUCCESS],\n",
    "        \"generation_attempts\": generation_attempts + 1,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ae5e8",
   "metadata": {},
   "source": [
    "## Evaluate tests node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfffd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import TypedDict, Dict, Any, List, Optional\n",
    "\n",
    "\n",
    "EVALUATE_LLM_NODE_NAME = \"Evaluate Tests Quality (LLM)\"\n",
    "# EVALUATE_LLM_FLOW_ASSESSMENT_MET = f\"{EVALUATE_LLM_NODE_NAME}: Test quality assessment target met\"\n",
    "# EVALUATE_LLM_FLOW_ASSESSMENT_NOT_MET = f\"{EVALUATE_LLM_NODE_NAME}: Test quality assessment target NOT met\"\n",
    "EVALUATE_LLM_FLOW_ASSESSMENT_GENERATED = f\"{EVALUATE_LLM_NODE_NAME}: Test quality assessment generated with qualitative_assessment\"\n",
    "EVALUATE_LLM_FLOW_MAX_ATTEMPTS_REACHED = f\"{EVALUATE_LLM_NODE_NAME}: Max attempts reached, assessment target NOT met\" \n",
    "EVALUATE_LLM_FLOW_FAILED_NO_TESTS = f\"{EVALUATE_LLM_NODE_NAME}: Failed (No generated tests to evaluate)\"\n",
    "EVALUATE_LLM_FLOW_FAILED_NO_CODE = f\"{EVALUATE_LLM_NODE_NAME}: Failed (No original code to evaluate against)\"\n",
    "EVALUATE_LLM_FLOW_FAILED_NO_ANALYSIS = f\"{EVALUATE_LLM_NODE_NAME}: Failed (No code analysis provided for evaluation)\"\n",
    "EVALUATE_LLM_FLOW_FAILED_LLM_EVAL = f\"{EVALUATE_LLM_NODE_NAME}: Failed (LLM evaluation error)\"\n",
    "EVALUATE_LLM_FLOW_FAILED_UNEXPECTED = f\"{EVALUATE_LLM_NODE_NAME}: Failed (Unexpected Error)\"\n",
    "\n",
    "MESSAGE_NO_GENERATED_TESTS = \"No generated test code found in state. Cannot evaluate.\" # Cập nhật message\n",
    "MESSAGE_NO_ORIGINAL_CODE_FOR_EVALUATION = \"No original code found in state. Cannot evaluate.\" # Cập nhật message\n",
    "MESSAGE_NO_CODE_ANALYSIS_FOR_EVALUATION = \"Code analysis is missing, which is crucial for LLM-based test evaluation.\"\n",
    "MESSAGE_LLM_EVALUATION_ERROR = \"An error occurred during LLM-based test evaluation.\"\n",
    "\n",
    "def evaluate_tests_llm_node(state: UnitTestWorkflowState, eval_chain) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates the generated test code using an LLM to assess its quality,\n",
    "    completeness, and likely coverage against the original code and its analysis.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = state.get('messages', [])\n",
    "    test_code: Optional[str] = state.get('test_code')\n",
    "    original_code: Optional[str] = state.get('original_code')\n",
    "    analyzed_code: Optional[Dict[str, Any]] = state.get('analyzed_code')\n",
    "    generation_attempts: int = state.get('generation_attempts')\n",
    "    max_generation_attempts: int = state.get('max_generation_attempts', 3)\n",
    "    current_flow: List[str] = state.get('flow', [])\n",
    "    \n",
    "    if not original_code:\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=MESSAGE_NO_ORIGINAL_CODE_FOR_EVALUATION)],\n",
    "            \"flow\": [EVALUATE_LLM_FLOW_FAILED_NO_CODE],\n",
    "        }\n",
    "    if not test_code:\n",
    "        if generation_attempts>= max_generation_attempts:\n",
    "            error_msg = f\"{error_msg} Max attempts ({max_generation_attempts}) reached.\"\n",
    "        else:\n",
    "            error_msg = MESSAGE_NO_GENERATED_TESTS\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=error_msg)],\n",
    "            \"flow\": [EVALUATE_LLM_FLOW_FAILED_NO_TESTS],\n",
    "        }\n",
    "    if not analyzed_code:\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=MESSAGE_NO_CODE_ANALYSIS_FOR_EVALUATION)],\n",
    "            \"flow\": [EVALUATE_LLM_FLOW_FAILED_NO_ANALYSIS],\n",
    "        }\n",
    "    try: \n",
    "        code_analysis_str = json.dumps(analyzed_code, indent=2)\n",
    "    except TypeError as e:\n",
    "        error_msg = f\"Could not serialize analyzed_code to JSON: {e}\"\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=error_msg)],\n",
    "            \"flow\": [EVALUATE_LLM_FLOW_FAILED_UNEXPECTED],\n",
    "        }\n",
    "        \n",
    "    llm_eval_result = eval_chain.invoke(\n",
    "        {\n",
    "        \"original_code\": original_code,\n",
    "        \"code_analysis_json\": str(analyzed_code),\n",
    "        \"test_code\": test_code\n",
    "        }\n",
    "    )\n",
    "    if not llm_eval_result or not isinstance(llm_eval_result, dict):\n",
    "        error_msg = \"LLM evaluation returned no data or invalid format.\"\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=error_msg)],\n",
    "            \"flow\": [EVALUATE_LLM_FLOW_FAILED_LLM_EVAL],\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=json.dumps(llm_eval_result))],\n",
    "        \"evaluation\": llm_eval_result,\n",
    "        \"flow\": [f\"{EVALUATE_LLM_FLOW_ASSESSMENT_GENERATED} [{llm_eval_result.get('qualitative_assessment')}]\" ],\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b255af",
   "metadata": {},
   "source": [
    "# Test Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2984c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = ''' from langchain.chains.llm import LLMChain\n",
    "from langchain.schema.messages import AIMessage\n",
    "from core.utils.schema import Code\n",
    "from core.graph.utils.state import CodeGenState\n",
    "from typing_extensions import List\n",
    "from langchain.schema.messages import BaseMessage\n",
    "\n",
    "STEP_NAME = \"Reflect\"\n",
    "def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:\n",
    "    \"\"\"\n",
    "    Performs a reflection step upon encountering an error during code generation.\n",
    "\n",
    "    This node is typically triggered after a failed `code_check`. It invokes\n",
    "    the provided `code_gen_chain` with the current message history (which should\n",
    "    include the error message from the check) and context.\n",
    "\n",
    "    It assumes the `code_gen_chain`, when prompted with the error context,\n",
    "    will provide reflective text or analysis within the `prefix` field of its\n",
    "    structured `Code` output.\n",
    "\n",
    "    This reflection text is then appended to the message history as a new\n",
    "    `AIMessage`, preserving the conversation context for the subsequent\n",
    "    generation attempt. The iteration count remains unchanged.\n",
    "\n",
    "    Args:\n",
    "        state: The current graph state, containing messages, iterations,\n",
    "            documentation, and previous generations.\n",
    "        code_gen_chain: The LLMChain instance configured to generate\n",
    "                        structured `Code` output. It's reused here for reflection.\n",
    "        framework: The target coding framework (e.g., 'python').\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing updates for the graph state:\n",
    "        - 'messages': The original messages list appended with the new\n",
    "                    reflection AIMessage.\n",
    "        - 'flow': A list containing the name of this node (\"Reflect\").\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = state['messages']\n",
    "    documentation_list: List[str] = [doc.page_content for doc in state.get('documentation', []) if hasattr(doc, 'page_content')]\n",
    "    documentation: str = \"\\n\".join(documentation_list)\n",
    "\n",
    "    reflection_code: Code = code_gen_chain.invoke(\n",
    "        {\"context\": documentation, \"question\": messages, \"framework\": framework}\n",
    "    )\n",
    "    reflection_message = AIMessage(\n",
    "        content=f\"Reflection on the error: {reflection_code.prefix}\"\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": reflection_message, \n",
    "        \"flow\": [STEP_NAME]\n",
    "        }\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f312f6c",
   "metadata": {},
   "source": [
    "## Test Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff60ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "    merged_dict = {}\n",
    "    for key, value in dict1.items():\n",
    "        merged_dict[key] = value\n",
    "    for key, value in dict2.items():\n",
    "        if key in merged_dict:\n",
    "            if isinstance(merged_dict[key], str) and isinstance(value, str):\n",
    "                merged_dict[key] += value\n",
    "            elif isinstance(merged_dict[key], list) and isinstance(value, list):\n",
    "                merged_dict[key].extend(value)\n",
    "            else:\n",
    "                merged_dict[key] = value\n",
    "        else:\n",
    "            merged_dict[key] = value\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "406d761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "input_state = UnitTestWorkflowState(\n",
    "    original_code=code,\n",
    "    analyzed_code=\"\",\n",
    "    flow=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "512991b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chains \n",
    "code_analysis_chain = create_code_analysis_chain(model=\"gemini-2.0-flash\",temperature=0.0)\n",
    "test_generation_chain = create_test_generation_chain(model=\"gemini-2.0-flash\",temperature=0.0)\n",
    "eval_chain = create_evaluation_chain(model=\"gemini-2.0-flash\",temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7bd15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_analysis_state = code_analysis(input_state, code_analysis_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1bbd962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzed_code': {'summary': 'The code defines a reflection step in a code generation process. It uses an LLMChain to generate a reflection on an error encountered during code generation and appends this reflection to the message history.',\n",
       "  'components': [{'type': 'function',\n",
       "    'name': 'reflect',\n",
       "    'description': 'Performs a reflection step upon encountering an error during code generation. It invokes the provided `code_gen_chain` with the current message history (which should include the error message from the check) and context. The reflection text is then appended to the message history as a new `AIMessage`, preserving the conversation context for the subsequent generation attempt.',\n",
       "    'signature': 'def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:',\n",
       "    'parameters': [{'name': 'state', 'type': 'CodeGenState'},\n",
       "     {'name': 'code_gen_chain', 'type': 'LLMChain'},\n",
       "     {'name': 'framework', 'type': 'str'}],\n",
       "    'returns': 'dict',\n",
       "    'key_behaviors': ['Extracts documentation from the state.',\n",
       "     'Invokes the code_gen_chain with context, question (messages), and framework.',\n",
       "     \"Creates an AIMessage containing the reflection from the code_gen_chain's output.\",\n",
       "     'Returns a dictionary containing the updated messages (with the reflection) and the flow (STEP_NAME).'],\n",
       "    'edge_cases': ['Empty documentation list in the state.',\n",
       "     'code_gen_chain returns a Code object with an empty or None prefix.',\n",
       "     \"The 'documentation' attribute in state is missing or not a list.\",\n",
       "     \"The elements in 'documentation' list do not have 'page_content' attribute.\"]}],\n",
       "  'dependencies': ['langchain.chains.llm.LLMChain',\n",
       "   'langchain.schema.messages.AIMessage',\n",
       "   'core.utils.schema.Code',\n",
       "   'core.graph.utils.state.CodeGenState',\n",
       "   'typing_extensions.List',\n",
       "   'langchain.schema.messages.BaseMessage']},\n",
       " 'messages': [AIMessage(content='{\\'summary\\': \\'The code defines a reflection step in a code generation process. It uses an LLMChain to generate a reflection on an error encountered during code generation and appends this reflection to the message history.\\', \\'components\\': [{\\'type\\': \\'function\\', \\'name\\': \\'reflect\\', \\'description\\': \\'Performs a reflection step upon encountering an error during code generation. It invokes the provided `code_gen_chain` with the current message history (which should include the error message from the check) and context. The reflection text is then appended to the message history as a new `AIMessage`, preserving the conversation context for the subsequent generation attempt.\\', \\'signature\\': \\'def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:\\', \\'parameters\\': [{\\'name\\': \\'state\\', \\'type\\': \\'CodeGenState\\'}, {\\'name\\': \\'code_gen_chain\\', \\'type\\': \\'LLMChain\\'}, {\\'name\\': \\'framework\\', \\'type\\': \\'str\\'}], \\'returns\\': \\'dict\\', \\'key_behaviors\\': [\\'Extracts documentation from the state.\\', \\'Invokes the code_gen_chain with context, question (messages), and framework.\\', \"Creates an AIMessage containing the reflection from the code_gen_chain\\'s output.\", \\'Returns a dictionary containing the updated messages (with the reflection) and the flow (STEP_NAME).\\'], \\'edge_cases\\': [\\'Empty documentation list in the state.\\', \\'code_gen_chain returns a Code object with an empty or None prefix.\\', \"The \\'documentation\\' attribute in state is missing or not a list.\", \"The elements in \\'documentation\\' list do not have \\'page_content\\' attribute.\"]}], \\'dependencies\\': [\\'langchain.chains.llm.LLMChain\\', \\'langchain.schema.messages.AIMessage\\', \\'core.utils.schema.Code\\', \\'core.graph.utils.state.CodeGenState\\', \\'typing_extensions.List\\', \\'langchain.schema.messages.BaseMessage\\']}', additional_kwargs={}, response_metadata={})],\n",
       " 'flow': ['Analyze Code Node: Analysis successful']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_analysis_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d38c41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation_state = generate_tests_node(code_analysis_state,test_generation_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c0cb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n', additional_kwargs={}, response_metadata={})],\n",
       " 'test_code': ['```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n'],\n",
       " 'flow': ['Generate Tests Node: Test generation successful'],\n",
       " 'generation_attempts': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generation_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4a4b9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_code': ' from langchain.chains.llm import LLMChain\\nfrom langchain.schema.messages import AIMessage\\nfrom core.utils.schema import Code\\nfrom core.graph.utils.state import CodeGenState\\nfrom typing_extensions import List\\nfrom langchain.schema.messages import BaseMessage\\n\\nSTEP_NAME = \"Reflect\"\\ndef reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:\\n    \"\"\"\\n    Performs a reflection step upon encountering an error during code generation.\\n\\n    This node is typically triggered after a failed `code_check`. It invokes\\n    the provided `code_gen_chain` with the current message history (which should\\n    include the error message from the check) and context.\\n\\n    It assumes the `code_gen_chain`, when prompted with the error context,\\n    will provide reflective text or analysis within the `prefix` field of its\\n    structured `Code` output.\\n\\n    This reflection text is then appended to the message history as a new\\n    `AIMessage`, preserving the conversation context for the subsequent\\n    generation attempt. The iteration count remains unchanged.\\n\\n    Args:\\n        state: The current graph state, containing messages, iterations,\\n            documentation, and previous generations.\\n        code_gen_chain: The LLMChain instance configured to generate\\n                        structured `Code` output. It\\'s reused here for reflection.\\n        framework: The target coding framework (e.g., \\'python\\').\\n\\n    Returns:\\n        A dictionary containing updates for the graph state:\\n        - \\'messages\\': The original messages list appended with the new\\n                    reflection AIMessage.\\n        - \\'flow\\': A list containing the name of this node (\"Reflect\").\\n    \"\"\"\\n    messages: List[BaseMessage] = state[\\'messages\\']\\n    documentation_list: List[str] = [doc.page_content for doc in state.get(\\'documentation\\', []) if hasattr(doc, \\'page_content\\')]\\n    documentation: str = \"\\n\".join(documentation_list)\\n\\n    reflection_code: Code = code_gen_chain.invoke(\\n        {\"context\": documentation, \"question\": messages, \"framework\": framework}\\n    )\\n    reflection_message = AIMessage(\\n        content=f\"Reflection on the error: {reflection_code.prefix}\"\\n    )\\n    return {\\n        \"messages\": reflection_message, \\n        \"flow\": [STEP_NAME]\\n        }\\n',\n",
       " 'analyzed_code': {'summary': 'The code defines a reflection step in a code generation process. It uses an LLMChain to generate a reflection on an error encountered during code generation and appends this reflection to the message history.',\n",
       "  'components': [{'type': 'function',\n",
       "    'name': 'reflect',\n",
       "    'description': 'Performs a reflection step upon encountering an error during code generation. It invokes the provided `code_gen_chain` with the current message history (which should include the error message from the check) and context. The reflection text is then appended to the message history as a new `AIMessage`, preserving the conversation context for the subsequent generation attempt.',\n",
       "    'signature': 'def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:',\n",
       "    'parameters': [{'name': 'state', 'type': 'CodeGenState'},\n",
       "     {'name': 'code_gen_chain', 'type': 'LLMChain'},\n",
       "     {'name': 'framework', 'type': 'str'}],\n",
       "    'returns': 'dict',\n",
       "    'key_behaviors': ['Extracts documentation from the state.',\n",
       "     'Invokes the code_gen_chain with context, question (messages), and framework.',\n",
       "     \"Creates an AIMessage containing the reflection from the code_gen_chain's output.\",\n",
       "     'Returns a dictionary containing the updated messages (with the reflection) and the flow (STEP_NAME).'],\n",
       "    'edge_cases': ['Empty documentation list in the state.',\n",
       "     'code_gen_chain returns a Code object with an empty or None prefix.',\n",
       "     \"The 'documentation' attribute in state is missing or not a list.\",\n",
       "     \"The elements in 'documentation' list do not have 'page_content' attribute.\"]}],\n",
       "  'dependencies': ['langchain.chains.llm.LLMChain',\n",
       "   'langchain.schema.messages.AIMessage',\n",
       "   'core.utils.schema.Code',\n",
       "   'core.graph.utils.state.CodeGenState',\n",
       "   'typing_extensions.List',\n",
       "   'langchain.schema.messages.BaseMessage']},\n",
       " 'flow': ['Analyze Code Node: Analysis successful',\n",
       "  'Generate Tests Node: Test generation successful'],\n",
       " 'messages': [AIMessage(content='{\\'summary\\': \\'The code defines a reflection step in a code generation process. It uses an LLMChain to generate a reflection on an error encountered during code generation and appends this reflection to the message history.\\', \\'components\\': [{\\'type\\': \\'function\\', \\'name\\': \\'reflect\\', \\'description\\': \\'Performs a reflection step upon encountering an error during code generation. It invokes the provided `code_gen_chain` with the current message history (which should include the error message from the check) and context. The reflection text is then appended to the message history as a new `AIMessage`, preserving the conversation context for the subsequent generation attempt.\\', \\'signature\\': \\'def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:\\', \\'parameters\\': [{\\'name\\': \\'state\\', \\'type\\': \\'CodeGenState\\'}, {\\'name\\': \\'code_gen_chain\\', \\'type\\': \\'LLMChain\\'}, {\\'name\\': \\'framework\\', \\'type\\': \\'str\\'}], \\'returns\\': \\'dict\\', \\'key_behaviors\\': [\\'Extracts documentation from the state.\\', \\'Invokes the code_gen_chain with context, question (messages), and framework.\\', \"Creates an AIMessage containing the reflection from the code_gen_chain\\'s output.\", \\'Returns a dictionary containing the updated messages (with the reflection) and the flow (STEP_NAME).\\'], \\'edge_cases\\': [\\'Empty documentation list in the state.\\', \\'code_gen_chain returns a Code object with an empty or None prefix.\\', \"The \\'documentation\\' attribute in state is missing or not a list.\", \"The elements in \\'documentation\\' list do not have \\'page_content\\' attribute.\"]}], \\'dependencies\\': [\\'langchain.chains.llm.LLMChain\\', \\'langchain.schema.messages.AIMessage\\', \\'core.utils.schema.Code\\', \\'core.graph.utils.state.CodeGenState\\', \\'typing_extensions.List\\', \\'langchain.schema.messages.BaseMessage\\']}', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n', additional_kwargs={}, response_metadata={})],\n",
       " 'test_code': ['```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n'],\n",
       " 'generation_attempts': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concated_state = merge_dict(input_state, code_analysis_state)\n",
    "concated_state = merge_dict(concated_state, test_generation_state)\n",
    "concated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b65172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='{\"qualitative_assessment\": \"medium\", \"confidence_score\": 6.0, \"positive_feedback\": [\"The tests cover basic functionality, including cases with empty documentation, empty reflection string, missing documentation attribute, and missing \\'page_content\\' within documentation elements.\", \"The tests use mocks effectively to isolate the `reflect` function and control the behavior of the `code_gen_chain`.\"], \"areas_for_improvement\": [\"The tests do not verify the arguments passed to `code_gen_chain.invoke`. It\\'s important to ensure the context, question, and framework are passed correctly.\", \"The tests use `CodeGenState` directly, which might not be the intended usage. It should be initialized with the correct parameters, or a mock should be used to control its behavior more precisely.\", \"The tests do not assert the type of the returned \\'messages\\'. It should be a list containing an `AIMessage`.\", \"The tests do not check if the documentation is correctly extracted and joined into a single string.\", \"The tests do not handle the case where the \\'documentation\\' attribute in the state is not a list. It assumes it\\'s always a list.\", \"The tests do not verify that the iteration count remains unchanged, as mentioned in the original function\\'s documentation.\", \"The tests use `mock_code_gen_chain.run.return_value` which is incorrect. The `code_gen_chain` is an `LLMChain` and the `invoke` method should be mocked, and it should return a `Code` object, not a string.\"], \"other_suggestions\": [\"Consider using more descriptive test names to clearly indicate the specific scenario being tested.\", \"Add assertions to verify the content of the messages passed to the `code_gen_chain`.\", \"Create a test case where the documentation list contains elements that are not strings or do not have the \\'page_content\\' attribute to ensure proper error handling or default behavior.\", \"The tests import `CodeGenState` and `reflect` from `promptflow.tools.open_model_llm`. This seems incorrect based on the original code which imports `CodeGenState` from `core.graph.utils.state` and `reflect` is defined in the current file. The import paths should be corrected.\", \"The tests should assert that the returned \\'messages\\' is a list containing the original messages and the new reflection message. Currently, it only checks the length and the last message.\"]}', additional_kwargs={}, response_metadata={})],\n",
       " 'evaluation': {'qualitative_assessment': 'medium',\n",
       "  'confidence_score': 6.0,\n",
       "  'positive_feedback': [\"The tests cover basic functionality, including cases with empty documentation, empty reflection string, missing documentation attribute, and missing 'page_content' within documentation elements.\",\n",
       "   'The tests use mocks effectively to isolate the `reflect` function and control the behavior of the `code_gen_chain`.'],\n",
       "  'areas_for_improvement': [\"The tests do not verify the arguments passed to `code_gen_chain.invoke`. It's important to ensure the context, question, and framework are passed correctly.\",\n",
       "   'The tests use `CodeGenState` directly, which might not be the intended usage. It should be initialized with the correct parameters, or a mock should be used to control its behavior more precisely.',\n",
       "   \"The tests do not assert the type of the returned 'messages'. It should be a list containing an `AIMessage`.\",\n",
       "   'The tests do not check if the documentation is correctly extracted and joined into a single string.',\n",
       "   \"The tests do not handle the case where the 'documentation' attribute in the state is not a list. It assumes it's always a list.\",\n",
       "   \"The tests do not verify that the iteration count remains unchanged, as mentioned in the original function's documentation.\",\n",
       "   'The tests use `mock_code_gen_chain.run.return_value` which is incorrect. The `code_gen_chain` is an `LLMChain` and the `invoke` method should be mocked, and it should return a `Code` object, not a string.'],\n",
       "  'other_suggestions': ['Consider using more descriptive test names to clearly indicate the specific scenario being tested.',\n",
       "   'Add assertions to verify the content of the messages passed to the `code_gen_chain`.',\n",
       "   \"Create a test case where the documentation list contains elements that are not strings or do not have the 'page_content' attribute to ensure proper error handling or default behavior.\",\n",
       "   'The tests import `CodeGenState` and `reflect` from `promptflow.tools.open_model_llm`. This seems incorrect based on the original code which imports `CodeGenState` from `core.graph.utils.state` and `reflect` is defined in the current file. The import paths should be corrected.',\n",
       "   \"The tests should assert that the returned 'messages' is a list containing the original messages and the new reflection message. Currently, it only checks the length and the last message.\"]},\n",
       " 'flow': ['Evaluate Tests Quality (LLM): Test quality assessment generated with qualitative_assessment [medium]']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_state = evaluate_tests_llm_node(\n",
    "    concated_state,\n",
    "    eval_chain\n",
    ")\n",
    "eval_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156be4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_code': ' from langchain.chains.llm import LLMChain\\nfrom langchain.schema.messages import AIMessage\\nfrom core.utils.schema import Code\\nfrom core.graph.utils.state import CodeGenState\\nfrom typing_extensions import List\\nfrom langchain.schema.messages import BaseMessage\\n\\nSTEP_NAME = \"Reflect\"\\ndef reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:\\n    \"\"\"\\n    Performs a reflection step upon encountering an error during code generation.\\n\\n    This node is typically triggered after a failed `code_check`. It invokes\\n    the provided `code_gen_chain` with the current message history (which should\\n    include the error message from the check) and context.\\n\\n    It assumes the `code_gen_chain`, when prompted with the error context,\\n    will provide reflective text or analysis within the `prefix` field of its\\n    structured `Code` output.\\n\\n    This reflection text is then appended to the message history as a new\\n    `AIMessage`, preserving the conversation context for the subsequent\\n    generation attempt. The iteration count remains unchanged.\\n\\n    Args:\\n        state: The current graph state, containing messages, iterations,\\n            documentation, and previous generations.\\n        code_gen_chain: The LLMChain instance configured to generate\\n                        structured `Code` output. It\\'s reused here for reflection.\\n        framework: The target coding framework (e.g., \\'python\\').\\n\\n    Returns:\\n        A dictionary containing updates for the graph state:\\n        - \\'messages\\': The original messages list appended with the new\\n                    reflection AIMessage.\\n        - \\'flow\\': A list containing the name of this node (\"Reflect\").\\n    \"\"\"\\n    messages: List[BaseMessage] = state[\\'messages\\']\\n    documentation_list: List[str] = [doc.page_content for doc in state.get(\\'documentation\\', []) if hasattr(doc, \\'page_content\\')]\\n    documentation: str = \"\\n\".join(documentation_list)\\n\\n    reflection_code: Code = code_gen_chain.invoke(\\n        {\"context\": documentation, \"question\": messages, \"framework\": framework}\\n    )\\n    reflection_message = AIMessage(\\n        content=f\"Reflection on the error: {reflection_code.prefix}\"\\n    )\\n    return {\\n        \"messages\": reflection_message, \\n        \"flow\": [STEP_NAME]\\n        }\\n',\n",
       " 'analyzed_code': {'summary': 'The code defines a reflection step in a code generation process. It uses an LLMChain to generate a reflection on an error encountered during code generation and appends this reflection to the message history.',\n",
       "  'components': [{'type': 'function',\n",
       "    'name': 'reflect',\n",
       "    'description': 'Performs a reflection step upon encountering an error during code generation. It invokes the provided `code_gen_chain` with the current message history (which should include the error message from the check) and context. The reflection text is then appended to the message history as a new `AIMessage`, preserving the conversation context for the subsequent generation attempt.',\n",
       "    'signature': 'def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:',\n",
       "    'parameters': [{'name': 'state', 'type': 'CodeGenState'},\n",
       "     {'name': 'code_gen_chain', 'type': 'LLMChain'},\n",
       "     {'name': 'framework', 'type': 'str'}],\n",
       "    'returns': 'dict',\n",
       "    'key_behaviors': ['Extracts documentation from the state.',\n",
       "     'Invokes the code_gen_chain with context, question (messages), and framework.',\n",
       "     \"Creates an AIMessage containing the reflection from the code_gen_chain's output.\",\n",
       "     'Returns a dictionary containing the updated messages (with the reflection) and the flow (STEP_NAME).'],\n",
       "    'edge_cases': ['Empty documentation list in the state.',\n",
       "     'code_gen_chain returns a Code object with an empty or None prefix.',\n",
       "     \"The 'documentation' attribute in state is missing or not a list.\",\n",
       "     \"The elements in 'documentation' list do not have 'page_content' attribute.\"]}],\n",
       "  'dependencies': ['langchain.chains.llm.LLMChain',\n",
       "   'langchain.schema.messages.AIMessage',\n",
       "   'core.utils.schema.Code',\n",
       "   'core.graph.utils.state.CodeGenState',\n",
       "   'typing_extensions.List',\n",
       "   'langchain.schema.messages.BaseMessage']},\n",
       " 'flow': ['Analyze Code Node: Analysis successful',\n",
       "  'Generate Tests Node: Test generation successful',\n",
       "  'Generate Tests Node: Test generation successful'],\n",
       " 'messages': [AIMessage(content='{\\'summary\\': \\'The code defines a reflection step in a code generation process. It uses an LLMChain to generate a reflection on an error encountered during code generation and appends this reflection to the message history.\\', \\'components\\': [{\\'type\\': \\'function\\', \\'name\\': \\'reflect\\', \\'description\\': \\'Performs a reflection step upon encountering an error during code generation. It invokes the provided `code_gen_chain` with the current message history (which should include the error message from the check) and context. The reflection text is then appended to the message history as a new `AIMessage`, preserving the conversation context for the subsequent generation attempt.\\', \\'signature\\': \\'def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:\\', \\'parameters\\': [{\\'name\\': \\'state\\', \\'type\\': \\'CodeGenState\\'}, {\\'name\\': \\'code_gen_chain\\', \\'type\\': \\'LLMChain\\'}, {\\'name\\': \\'framework\\', \\'type\\': \\'str\\'}], \\'returns\\': \\'dict\\', \\'key_behaviors\\': [\\'Extracts documentation from the state.\\', \\'Invokes the code_gen_chain with context, question (messages), and framework.\\', \"Creates an AIMessage containing the reflection from the code_gen_chain\\'s output.\", \\'Returns a dictionary containing the updated messages (with the reflection) and the flow (STEP_NAME).\\'], \\'edge_cases\\': [\\'Empty documentation list in the state.\\', \\'code_gen_chain returns a Code object with an empty or None prefix.\\', \"The \\'documentation\\' attribute in state is missing or not a list.\", \"The elements in \\'documentation\\' list do not have \\'page_content\\' attribute.\"]}], \\'dependencies\\': [\\'langchain.chains.llm.LLMChain\\', \\'langchain.schema.messages.AIMessage\\', \\'core.utils.schema.Code\\', \\'core.graph.utils.state.CodeGenState\\', \\'typing_extensions.List\\', \\'langchain.schema.messages.BaseMessage\\']}', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n', additional_kwargs={}, response_metadata={})],\n",
       " 'test_code': ['```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n',\n",
       "  '```python\\nimport unittest\\nfrom unittest.mock import MagicMock, patch\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"This is a reflection.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no docs.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[])\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_string(self):\\n        \"\"\"Tests reflect when code_gen_chain returns an empty string.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 2)\\n        self.assertEqual(result[\"messages\"][-1].content, \"\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n\\n    def test_reflect_missing_documentation_attribute(self):\\n        \"\"\"Tests reflect when the \\'documentation\\' attribute is missing in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing docs.\"\\n        state = CodeGenState(messages=[\"initial message\"])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(AttributeError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with missing page content.\"\\n        state = CodeGenState(messages=[\"initial message\"], documentation=[{\"other_content\": \"some content\"}])\\n        framework = \"test_framework\"\\n\\n        with self.assertRaises(KeyError):\\n            reflect(state, mock_code_gen_chain, framework)\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        from promptflow.tools.open_model_llm import CodeGenState, reflect\\n        mock_code_gen_chain = MagicMock()\\n        mock_code_gen_chain.run.return_value = \"Reflection with no messages.\"\\n        state = CodeGenState(messages=[], documentation=[\"doc1\", \"doc2\"])\\n        state.documentation = [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertEqual(len(result[\"messages\"]), 1)\\n        self.assertEqual(result[\"messages\"][-1].content, \"Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], \"REFLECTION\")\\n        mock_code_gen_chain.run.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n'],\n",
       " 'generation_attempts': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_state = merge_dict(concated_state, test_generation_state)\n",
    "concated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f5fd2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='```python\\nimport unittest\\nfrom unittest.mock import MagicMock\\nfrom langchain.schema.messages import AIMessage, BaseMessage\\nfrom core.utils.schema import Code\\nfrom core.graph.utils.state import CodeGenState\\nfrom typing import List, Dict, Any\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"This is a reflection.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with no docs.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': []}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_prefix(self):\\n        \"\"\"Tests reflect when code_gen_chain returns a Code object with an empty prefix.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: \")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with missing page content.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': [{\"other_content\": \"some content\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with missing page content.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with no messages.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [], \\'documentation\\': [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_documentation_is_none(self):\\n        \"\"\"Tests reflect when documentation is None in the state.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with no docs.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': None}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_documentation_not_a_list(self):\\n        \"\"\"Tests reflect when documentation is not a list in the state.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with invalid docs.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': \"not a list\"}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with invalid docs.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n', additional_kwargs={}, response_metadata={})],\n",
       " 'test_code': ['```python\\nimport unittest\\nfrom unittest.mock import MagicMock\\nfrom langchain.schema.messages import AIMessage, BaseMessage\\nfrom core.utils.schema import Code\\nfrom core.graph.utils.state import CodeGenState\\nfrom typing import List, Dict, Any\\n\\nclass TestReflect(unittest.TestCase):\\n\\n    def test_reflect_basic(self):\\n        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"This is a reflection.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_empty_documentation(self):\\n        \"\"\"Tests reflect with an empty documentation list.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with no docs.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': []}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_code_gen_chain_returns_empty_prefix(self):\\n        \"\"\"Tests reflect when code_gen_chain returns a Code object with an empty prefix.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: \")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_documentation_element_missing_page_content(self):\\n        \"\"\"Tests reflect when an element in \\'documentation\\' is missing \\'page_content\\'.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with missing page content.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': [{\"other_content\": \"some content\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with missing page content.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_no_messages(self):\\n        \"\"\"Tests reflect with no messages in the state.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with no messages.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [], \\'documentation\\': [{\"page_content\": \"doc1\"}, {\"page_content\": \"doc2\"}]}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with no messages.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_documentation_is_none(self):\\n        \"\"\"Tests reflect when documentation is None in the state.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with no docs.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': None}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with no docs.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n\\n    def test_reflect_documentation_not_a_list(self):\\n        \"\"\"Tests reflect when documentation is not a list in the state.\"\"\"\\n        mock_code_gen_chain = MagicMock()\\n        mock_code = Code(prefix=\"Reflection with invalid docs.\", code=\"\")\\n        mock_code_gen_chain.invoke.return_value = mock_code\\n        state: Dict[str, Any] = {\\'messages\\': [AIMessage(content=\"initial message\")], \\'documentation\\': \"not a list\"}\\n        framework = \"test_framework\"\\n\\n        result = reflect(state, mock_code_gen_chain, framework)\\n\\n        self.assertIsInstance(result[\"messages\"], AIMessage)\\n        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: Reflection with invalid docs.\")\\n        self.assertEqual(result[\"flow\"], [\"Reflect\"])\\n        mock_code_gen_chain.invoke.assert_called_once()\\n```\\n\\n# You might want to add a way to run these tests if needed for standalone execution\\n# e.g., if __name__ == \\'__main__\\': unittest.main()\\n'],\n",
       " 'flow': ['Generate Tests Node: Test generation successful'],\n",
       " 'generation_attempts': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regenrate\n",
    "test_generation_state = generate_tests_node(concated_state,test_generation_chain)\n",
    "test_generation_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38d683",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8448d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "workflow = StateGraph(UnitTestWorkflowState)\n",
    "workflow.add_node(\"analyze_code_node\", \n",
    "                lambda state: code_analysis(state, code_analysis_chain))\n",
    "workflow.add_node(\"generate_tests_node\",\n",
    "                lambda state: generate_tests_node(state, test_generation_chain))\n",
    "workflow.add_node(\"evaluate_tests_node\",\n",
    "                lambda state: evaluate_tests_llm_node(state, eval_chain))\n",
    "workflow.add_edge(START, \"analyze_code_node\")\n",
    "workflow.add_edge(\"analyze_code_node\", \"generate_tests_node\")\n",
    "workflow.add_edge(\"generate_tests_node\", \"evaluate_tests_node\")\n",
    "workflow.add_edge(\"evaluate_tests_node\", END)\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate_tests_node\",\n",
    "    decision_to_end_workflow,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"regenerate\": \"generate_tests_node\",\n",
    "    }\n",
    ")\n",
    "workflow = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a33a77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_result= workflow.invoke(\n",
    "    {\n",
    "        \"original_code\": code,\n",
    "        \"analyzed_code\": \"\",\n",
    "        \"test_code\": \"\",\n",
    "        \"flow\": [],\n",
    "        \"max_generation_attempts\": 3,\n",
    "        \"generation_attempts\": 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec2c38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(state):\n",
    "    print(\"Flow:\")\n",
    "    for flow in state[\"flow\"]:\n",
    "        print(flow, end=\" -> \")\n",
    "    print(\"\\n\")\n",
    "    print(\"Messages:\")\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, AIMessage):\n",
    "            print(\"=\"*20, \"AIMessage\", \"=\"*20)\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(\"=\"*20, \"HumanMessage\", \"=\"*20)\n",
    "        if isinstance(message, SystemMessage):\n",
    "            print(\"=\"*20, \"SystemMessage\", \"=\"*20)\n",
    "        print(message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf6bbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow:\n",
      "Analyze Code Node: Analysis successful -> Generate Tests Node: Test generation successful -> Evaluate Tests Quality (LLM): Test quality assessment generated with qualitative_assessment [medium] -> Generate Tests Node: Test generation successful -> Evaluate Tests Quality (LLM): Test quality assessment generated with qualitative_assessment [medium] -> Generate Tests Node: Test generation successful -> Evaluate Tests Quality (LLM): Test quality assessment generated with qualitative_assessment [high] -> \n",
      "\n",
      "Messages:\n",
      "==================== AIMessage ====================\n",
      "{'summary': 'The code defines a reflection step in a code generation process. It uses an LLMChain to analyze errors and generate reflective text, which is then added to the message history to guide subsequent code generation attempts.', 'components': [{'type': 'function', 'name': 'reflect', 'description': 'Performs a reflection step upon encountering an error during code generation. It invokes the provided `code_gen_chain` with the current message history (which should include the error message from the check) and context. It assumes the `code_gen_chain`, when prompted with the error context, will provide reflective text or analysis within the `prefix` field of its structured `Code` output. This reflection text is then appended to the message history as a new `AIMessage`, preserving the conversation context for the subsequent generation attempt. The iteration count remains unchanged.', 'signature': 'def reflect(state: CodeGenState, code_gen_chain: LLMChain, framework: str) -> dict:', 'parameters': [{'name': 'state', 'type': 'CodeGenState'}, {'name': 'code_gen_chain', 'type': 'LLMChain'}, {'name': 'framework', 'type': 'str'}], 'returns': 'dict', 'key_behaviors': ['Extracts messages and documentation from the state.', 'Invokes the code_gen_chain with context, question (messages), and framework.', \"Creates an AIMessage containing the reflection from the code_gen_chain's output.\", 'Returns a dictionary containing the new reflection message and the flow step name.'], 'edge_cases': ['Empty documentation list in the state.', 'code_gen_chain returns a Code object with an empty or missing prefix.', \"The state does not contain 'documentation' key.\", \"The state does not contain 'messages' key.\"]}], 'dependencies': ['langchain.chains.llm.LLMChain', 'langchain.schema.messages.AIMessage', 'core.utils.schema.Code', 'core.graph.utils.state.CodeGenState', 'typing_extensions.List', 'langchain.schema.messages.BaseMessage']}\n",
      "==================== AIMessage ====================\n",
      "```python\n",
      "import unittest\n",
      "from unittest.mock import MagicMock\n",
      "from langchain.schema.messages import AIMessage, HumanMessage\n",
      "from core.utils.schema import Code\n",
      "from core.graph.utils.state import CodeGenState\n",
      "from typing import List\n",
      "from langchain.schema.document import Document\n",
      "\n",
      "STEP_NAME = \"Reflect\"\n",
      "\n",
      "class TestReflect(unittest.TestCase):\n",
      "\n",
      "    def test_reflect_basic(self):\n",
      "        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once()\n",
      "\n",
      "    def test_reflect_empty_documentation(self):\n",
      "        \"\"\"Tests reflect with an empty documentation list.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': []\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once()\n",
      "\n",
      "    def test_reflect_empty_prefix(self):\n",
      "        \"\"\"Tests reflect when the code_gen_chain returns a Code object with an empty prefix.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: \")\n",
      "        mock_code_gen_chain.invoke.assert_called_once()\n",
      "\n",
      "    def test_reflect_missing_documentation_key(self):\n",
      "        \"\"\"Tests reflect when the state does not contain the 'documentation' key.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once()\n",
      "\n",
      "    def test_reflect_missing_messages_key(self):\n",
      "        \"\"\"Tests reflect when the state does not contain the 'messages' key.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        with self.assertRaises(KeyError):\n",
      "            reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "    def test_reflect_documentation_no_page_content(self):\n",
      "        \"\"\"Tests reflect when documentation doesn't have page_content.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        mock_doc = MagicMock()\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [mock_doc]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once()\n",
      "```\n",
      "\n",
      "# You might want to add a way to run these tests if needed for standalone execution\n",
      "# e.g., if __name__ == '__main__': unittest.main()\n",
      "\n",
      "==================== AIMessage ====================\n",
      "{\"qualitative_assessment\": \"medium\", \"confidence_score\": 6.5, \"positive_feedback\": [\"The tests cover the basic functionality of the `reflect` function, including checking the returned dictionary structure and content.\", \"The tests address several edge cases identified in the analysis, such as empty documentation, missing documentation key, and empty prefix in the Code object.\", \"The tests use mocking effectively to isolate the `reflect` function and control the behavior of the `code_gen_chain`.\"], \"areas_for_improvement\": [\"The tests do not explicitly verify that the `code_gen_chain.invoke` method is called with the correct arguments (context, question, framework). While it asserts that it's called, it doesn't check the input.\", \"The tests only check for the presence of the 'messages' key in the result, but they don't verify that the original messages from the state are preserved. The current implementation replaces the messages instead of appending to them.\", \"There's no test case specifically for when the `documentation` list contains `Document` objects with empty `page_content` strings. This could lead to unexpected behavior when joining the documentation strings.\", \"The tests don't cover the scenario where the `code_gen_chain.invoke` method raises an exception. This is a potential failure point that should be tested.\", \"The tests assume that the 'messages' key always exists. While the test for missing 'messages' key raises an error, it might be more robust to handle this case gracefully within the `reflect` function itself (e.g., by returning an error or logging a warning).\"], \"other_suggestions\": [\"Consider adding assertions to verify the arguments passed to `mock_code_gen_chain.invoke` to ensure the function is being called with the expected inputs.\", \"Add a test case to verify that the documentation string is correctly constructed when multiple `Document` objects are present in the `documentation` list.\", \"The test names could be more descriptive. For example, `test_reflect_missing_documentation_key` could be renamed to `test_reflect_missing_documentation_key_raises_keyerror` if that's the expected behavior, or `test_reflect_missing_documentation_key_handles_gracefully` if the function is expected to handle the missing key without raising an error.\", \"Consider adding a test to check the type of the elements in the 'documentation' list. The current implementation assumes they are Document objects with a page_content attribute, but this might not always be the case.\"]}\n",
      "==================== AIMessage ====================\n",
      "```python\n",
      "import unittest\n",
      "from unittest.mock import MagicMock, call\n",
      "from langchain.schema.messages import AIMessage, HumanMessage\n",
      "from core.utils.schema import Code\n",
      "from core.graph.utils.state import CodeGenState\n",
      "from typing import List\n",
      "from langchain.schema.document import Document\n",
      "\n",
      "STEP_NAME = \"Reflect\"\n",
      "\n",
      "class TestReflect(unittest.TestCase):\n",
      "\n",
      "    def test_reflect_basic(self):\n",
      "        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is some documentation.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_empty_documentation(self):\n",
      "        \"\"\"Tests reflect with an empty documentation list.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': []\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_empty_prefix(self):\n",
      "        \"\"\"Tests reflect when the code_gen_chain returns a Code object with an empty prefix.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: \")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is some documentation.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_missing_documentation_key(self):\n",
      "        \"\"\"Tests reflect when the state does not contain the 'documentation' key.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_missing_messages_key(self):\n",
      "        \"\"\"Tests reflect when the state does not contain the 'messages' key.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        with self.assertRaises(KeyError):\n",
      "            reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "    def test_reflect_documentation_no_page_content(self):\n",
      "        \"\"\"Tests reflect when documentation doesn't have page_content.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        mock_doc = MagicMock()\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [mock_doc]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_multiple_documentation(self):\n",
      "        \"\"\"Tests reflect with multiple documentation entries.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [\n",
      "                Document(page_content=\"This is doc1.\"),\n",
      "                Document(page_content=\"This is doc2.\")\n",
      "            ]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is doc1.\\nThis is doc2.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_documentation_with_empty_page_content(self):\n",
      "        \"\"\"Tests reflect when documentation contains empty page_content.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [\n",
      "                Document(page_content=\"This is doc1.\"),\n",
      "                Document(page_content=\"\")\n",
      "            ]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is doc1.\\n\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_code_gen_chain_raises_exception(self):\n",
      "        \"\"\"Tests reflect when the code_gen_chain raises an exception.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code_gen_chain.invoke.side_effect = Exception(\"Code generation failed.\")\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        with self.assertRaises(Exception) as context:\n",
      "            reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertEqual(str(context.exception), \"Code generation failed.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is some documentation.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "```\n",
      "\n",
      "# You might want to add a way to run these tests if needed for standalone execution\n",
      "# e.g., if __name__ == '__main__': unittest.main()\n",
      "\n",
      "==================== AIMessage ====================\n",
      "{\"qualitative_assessment\": \"medium\", \"confidence_score\": 6.5, \"positive_feedback\": [\"The tests cover the basic functionality of the `reflect` function, including checking the returned dictionary structure and the `flow` value.\", \"The tests handle cases with empty documentation, missing documentation key, and empty prefix in the `Code` object.\", \"The tests correctly mock the `code_gen_chain` and assert that it's called with the expected arguments.\", \"The tests handle the case where the code_gen_chain raises an exception.\"], \"areas_for_improvement\": [\"The tests only use a single HumanMessage in the 'messages' list. It should test with multiple messages to ensure the question passed to the LLMChain is correctly formatted.\", \"The tests don't explicitly check the type of the elements within the 'documentation' list. It only checks for `page_content` attribute. It should verify that the elements are Document objects or at least have the expected attributes.\", \"The tests don't verify the content of the 'context' argument passed to `code_gen_chain.invoke` when documentation entries have special characters or are very long. It should test for proper escaping or handling of such cases.\", \"The tests don't check what happens if the `code_gen_chain` returns None or an invalid `Code` object (e.g., missing 'code' or 'language' attributes).\", \"The tests don't check the case where the documentation list contains elements that are not Document objects and do not have the `page_content` attribute. This could lead to an AttributeError.\"], \"other_suggestions\": [\"Consider adding more descriptive names to the test functions to improve readability.\", \"Add assertions to check the type of the 'messages' element in the state. It should be a list of BaseMessage objects.\", \"The test `test_reflect_missing_messages_key` correctly raises a KeyError, but it could be improved by checking the specific error message to ensure it's related to the missing 'messages' key.\", \"Consider adding a test case where the documentation contains a mix of valid and invalid Document objects (some with page_content, some without).\", \"The tests could benefit from using parameterized testing to reduce code duplication and improve maintainability, especially for testing different documentation scenarios.\"]}\n",
      "==================== AIMessage ====================\n",
      "```python\n",
      "import unittest\n",
      "from unittest.mock import MagicMock, call\n",
      "from langchain.schema.messages import AIMessage, HumanMessage, BaseMessage\n",
      "from core.utils.schema import Code\n",
      "from core.graph.utils.state import CodeGenState\n",
      "from typing import List\n",
      "from langchain.schema.document import Document\n",
      "from typing import Any\n",
      "\n",
      "STEP_NAME = \"Reflect\"\n",
      "\n",
      "class TestReflect(unittest.TestCase):\n",
      "\n",
      "    def test_reflect_basic(self):\n",
      "        \"\"\"Tests the basic functionality of the reflect function.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is some documentation.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_empty_documentation(self):\n",
      "        \"\"\"Tests reflect with an empty documentation list.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': []\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_empty_prefix(self):\n",
      "        \"\"\"Tests reflect when the code_gen_chain returns a Code object with an empty prefix.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: \")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is some documentation.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_missing_documentation_key(self):\n",
      "        \"\"\"Tests reflect when the state does not contain the \\'documentation\\' key.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_missing_messages_key(self):\n",
      "        \"\"\"Tests reflect when the state does not contain the \\'messages\\' key.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        with self.assertRaises(KeyError) as context:\n",
      "            reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertEqual(str(context.exception), \"'messages'\")\n",
      "        #mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is some documentation.\", \"question\": [], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_documentation_no_page_content(self):\n",
      "        \"\"\"Tests reflect when documentation doesn\\'t have page_content.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        mock_doc = MagicMock()\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [mock_doc]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_multiple_documentation(self):\n",
      "        \"\"\"Tests reflect with multiple documentation entries.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [\n",
      "                Document(page_content=\"This is doc1.\"),\n",
      "                Document(page_content=\"This is doc2.\")\n",
      "            ]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is doc1.\\nThis is doc2.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_documentation_with_empty_page_content(self):\n",
      "        \"\"\"Tests reflect when documentation contains empty page_content.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [\n",
      "                Document(page_content=\"This is doc1.\"),\n",
      "                Document(page_content=\"\")\n",
      "            ]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is doc1.\\n\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_code_gen_chain_raises_exception(self):\n",
      "        \"\"\"Tests reflect when the code_gen_chain raises an exception.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code_gen_chain.invoke.side_effect = Exception(\"Code generation failed.\")\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        with self.assertRaises(Exception) as context:\n",
      "            reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertEqual(str(context.exception), \"Code generation failed.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\"context\": \"This is some documentation.\", \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")], \"framework\": \"python\"})\n",
      "\n",
      "    def test_reflect_multiple_messages(self):\n",
      "        \"\"\"Tests reflect with multiple messages in the state.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [\n",
      "                HumanMessage(content=\"Write a function to add two numbers.\"),\n",
      "                AIMessage(content=\"Here's my attempt: ...\"),\n",
      "                HumanMessage(content=\"That's incorrect, try again.\")\n",
      "            ],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\n",
      "            \"context\": \"This is some documentation.\",\n",
      "            \"question\": [\n",
      "                HumanMessage(content=\"Write a function to add two numbers.\"),\n",
      "                AIMessage(content=\"Here's my attempt: ...\"),\n",
      "                HumanMessage(content=\"That's incorrect, try again.\")\n",
      "            ],\n",
      "            \"framework\": \"python\"\n",
      "        })\n",
      "\n",
      "    def test_reflect_documentation_special_characters(self):\n",
      "        \"\"\"Tests reflect with special characters in the documentation.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is doc with special chars: !@#$%^&*()_+=-`~[]\\{}|;':\\\",./<>?\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\n",
      "            \"context\": \"This is doc with special chars: !@#$%^&*()_+=-`~[]\\{}|;':\\\",./<>?\",\n",
      "            \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            \"framework\": \"python\"\n",
      "        })\n",
      "\n",
      "    def test_reflect_code_gen_chain_returns_none(self):\n",
      "        \"\"\"Tests reflect when the code_gen_chain returns None.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code_gen_chain.invoke.return_value = None\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        with self.assertRaises(AttributeError):\n",
      "            reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "    def test_reflect_documentation_invalid_type(self):\n",
      "        \"\"\"Tests reflect when documentation contains an element that is not a Document.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [\"This is not a document\"]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        with self.assertRaises(AttributeError):\n",
      "            reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "    def test_reflect_documentation_mixed_valid_invalid(self):\n",
      "        \"\"\"Tests reflect with a mix of valid and invalid Document objects in documentation.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        mock_doc = MagicMock()\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [\n",
      "                Document(page_content=\"This is doc1.\"),\n",
      "                mock_doc,\n",
      "                Document(page_content=\"This is doc2.\")\n",
      "            ]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\n",
      "            \"context\": \"This is doc1.\\n\\nThis is doc2.\",\n",
      "            \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            \"framework\": \"python\"\n",
      "        })\n",
      "\n",
      "    def test_reflect_messages_empty_list(self):\n",
      "        \"\"\"Tests reflect with an empty list of messages.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = Code(code=\"some code\", language=\"python\", prefix=\"This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\n",
      "            \"context\": \"This is some documentation.\",\n",
      "            \"question\": [],\n",
      "            \"framework\": \"python\"\n",
      "        })\n",
      "\n",
      "    def test_reflect_code_gen_chain_returns_code_missing_attributes(self):\n",
      "        \"\"\"Tests reflect when the code_gen_chain returns a Code object with missing attributes.\"\"\"\n",
      "        mock_code_gen_chain = MagicMock()\n",
      "        mock_code = MagicMock()\n",
      "        mock_code.prefix = \"This is a reflection.\"\n",
      "        mock_code_gen_chain.invoke.return_value = mock_code\n",
      "        state: CodeGenState = {\n",
      "            'messages': [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            'documentation': [Document(page_content=\"This is some documentation.\")]\n",
      "        }\n",
      "        framework = \"python\"\n",
      "\n",
      "        result = reflect(state, mock_code_gen_chain, framework)\n",
      "\n",
      "        self.assertIn(\"messages\", result)\n",
      "        self.assertIn(\"flow\", result)\n",
      "        self.assertEqual(result[\"flow\"], [STEP_NAME])\n",
      "        self.assertIsInstance(result[\"messages\"], AIMessage)\n",
      "        self.assertEqual(result[\"messages\"].content, \"Reflection on the error: This is a reflection.\")\n",
      "        mock_code_gen_chain.invoke.assert_called_once_with({\n",
      "            \"context\": \"This is some documentation.\",\n",
      "            \"question\": [HumanMessage(content=\"Write a function to add two numbers.\")],\n",
      "            \"framework\": \"python\"\n",
      "        })\n",
      "```\n",
      "\n",
      "# You might want to add a way to run these tests if needed for standalone execution\n",
      "# e.g., if __name__ == '__main__': unittest.main()\n",
      "\n",
      "==================== AIMessage ====================\n",
      "{\"qualitative_assessment\": \"high\", \"confidence_score\": 9.0, \"positive_feedback\": [\"The tests cover the basic functionality of the `reflect` function, including checking the returned dictionary structure and content.\", \"The tests handle edge cases such as empty documentation, missing documentation key, empty prefix in the Code object, and missing messages key.\", \"The tests correctly mock the `code_gen_chain` and its `invoke` method to isolate the `reflect` function.\", \"The tests include scenarios with multiple documentation entries and special characters in the documentation.\", \"The tests handle cases where the `code_gen_chain` raises an exception or returns None.\", \"The tests cover scenarios with multiple messages in the state and an empty list of messages.\", \"The tests address cases where documentation entries are not Document objects or have missing page_content attributes.\", \"The tests check for mixed valid and invalid Document objects in the documentation list.\"], \"areas_for_improvement\": [\"The tests do not explicitly assert that the iteration count remains unchanged, although this is mentioned in the function's docstring. A test case could be added to verify this.\", \"While the tests cover cases where documentation is missing or empty, they don't explicitly test the scenario where the documentation list contains `None` values. This could lead to unexpected behavior.\", \"The tests could benefit from more specific assertions about the arguments passed to `mock_code_gen_chain.invoke`. While the tests check that it's called with the correct context, question, and framework, they don't verify the exact content of the 'question' argument when multiple messages are present. This could be improved for better accuracy.\", \"The tests could include a scenario where the `Code` object returned by `code_gen_chain` has `None` values for attributes other than `prefix` (e.g., `code`, `language`). While the function only uses `prefix`, it's good practice to ensure that the code doesn't crash if other attributes are unexpectedly `None`.\"], \"other_suggestions\": [\"Consider adding a test case to verify that the `framework` argument is correctly passed to the `code_gen_chain.invoke` method.\", \"The test names are generally clear, but some could be slightly more descriptive to improve readability (e.g., `test_reflect_missing_documentation_key` could be `test_reflect_state_missing_documentation_key`).\", \"The tests use `MagicMock` effectively, but ensure that the mocked objects behave as expected in all scenarios. For example, if the `Code` object is expected to have specific attributes, ensure that the mock has those attributes defined.\", \"While the tests cover many edge cases, it's always a good idea to review the code again and consider any other potential scenarios that might not be covered. For example, what happens if the `state` dictionary itself is `None`?\"]}\n"
     ]
    }
   ],
   "source": [
    "pretty_print(wf_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
